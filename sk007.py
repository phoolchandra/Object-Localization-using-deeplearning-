# -*- coding: utf-8 -*-
"""sk007.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aTczNb5ebCbxJQrP_UlEYAj_pgZdWu_q

# Localisation of an object in am image using tensorflow, keras and google colab

Simply select "GPU" in the Accelerator drop-down in Notebook Settings (either through the Edit menu or the command palette at cmd/ctrl-shift-P).
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import cv2
import os

def generate_data(file_list, Y, batch_size):
    """Replaces Keras' native ImageDataGenerator."""
    i = 0
    while True:
        image_batch = []
        y_batch = []
        for b in range(batch_size):
            if i == len(file_list):
                i = 0
            sample = Y[i]
            #print(sample)
            cur = [sample[0]/480.0 , sample[1]/640.0 , sample[2]/480.0, sample[3]/640.0] 
            y_batch.append(cur)
            image = cv2.imread(os.path.join("/content/gdrive/My Drive/trainingimages", file_list[i]))
            image=cv2.resize(image,(128,128))
            image = np.resize(image,(128,128,1))
            image_batch.append(((image - 128) / 128.0))
            i += 1

        yield (np.array(image_batch), np.array(y_batch))

def generate_data_t(file_list, batch_size):
    """Replaces Keras' native ImageDataGenerator."""
    i = 0
    while True:
        image_batch = []
        for b in range(batch_size):
            if i == len(file_list):
                i = 0
            #print(sample)
            image = cv2.imread(os.path.join("/content/gdrive/My Drive/testingimages", file_list[i]))
            image = cv2.resize(image,(128,128))
            image = np.resize(image,(128,128,1))
            
            image_batch.append(((image - 128) / 128.0))
            i += 1
        yield np.array(image_batch)



# %cd

import numpy as np
from keras import layers
from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D
from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D
from keras.models import Model
from keras.preprocessing import image
from keras.utils import layer_utils
from keras.utils.data_utils import get_file
from keras.applications.imagenet_utils import preprocess_input
import pydot
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot
from keras.utils import plot_model
import keras.backend as K
K.set_image_data_format('channels_last')
import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow

# %matplotlib inline

from keras.engine.topology import Input
from keras.layers import BatchNormalization, Concatenate, Conv2D, Dense, Dropout, Flatten, MaxPooling2D
from keras.models import Model
# def build_model(with_dropout=True):
#     kwargs     = {'activation':'relu', 'padding':'same'}
#     conv_drop  = 0.2
#     dense_drop = 0.5
#     inp        = Input(shape=(128, 128, 1))

#     x = inp

#     x = Conv2D(64, (7, 7), strides = (1, 1), name = 'conv0')(x)
#     x = Conv2D(64, (2, 2), strides = (1, 1))(x)
#     x = BatchNormalization(axis = 3, name = 'bn0')(x)
#     x = Activation('relu')(x)

#     # MAXPOOL
#     x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)
#     x = Conv2D(64, (3, 3), **kwargs)(x)
#     x = Conv2D(64, (3, 3), **kwargs)(x)
#     x = BatchNormalization()(x)
#     if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)

#     x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)
#     x = Conv2D(64, (3, 3), **kwargs)(x)
#     x = Conv2D(64, (3, 3), **kwargs)(x)
#     x = BatchNormalization()(x)
#     if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)

#     x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)
#     x = Conv2D(64, (3, 3), **kwargs)(x)
#     x = Conv2D(64, (3, 3), **kwargs)(x)
#     x = BatchNormalization()(x)
#     if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)

#     x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)
#     x = Conv2D(64, (3, 3), **kwargs)(x)
#     x = Conv2D(64, (3, 3), **kwargs)(x)
#     x = BatchNormalization()(x)
#     if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)

#     x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)
#     x = Conv2D(64, (3, 3), **kwargs)(x)
#     x = Conv2D(64, (3, 3), **kwargs)(x)
#     x = BatchNormalization()(x)
#     if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)

#     x = Flatten()(x)
#     x = Dense(4, activation='linear', name='fc')(x)
#     return Model(inp,x)

# happyModel = build_model(with_dropout=True)
# happyModel.summary()

def HappyModel(input_shape):
    """
    Implementation of the HappyModel.
    
    Arguments:
    input_shape -- shape of the images of the dataset

    Returns:
    model -- a Model() instance in Keras
    """
    
    ### START CODE HERE ###
    # Feel free to use the suggested outline in the text above to get started, and run through the whole
    # exercise (including the later portions of this notebook) once. The come back also try out other
    # network architectures as well. 
    
    X_input = Input(input_shape)
    X = X_input
    # Zero-Padding: pads the border of X_input with zeroes
    X = ZeroPadding2D((3, 3))(X)

    # CONV -> BN -> RELU Block applied to X
    X = Conv2D(64, (7, 7), strides = (1, 1), name = 'conv0')(X)
    X = Conv2D(64, (2, 2), strides = (1, 1))(X)
    X = BatchNormalization(axis = 3, name = 'bn0')(X)
    X = Activation('relu')(X)

    # MAXPOOL
    X = AveragePooling2D((4, 4), name='max_pool')(X)
    
    
    
    
    
    
    
    X = ZeroPadding2D((3, 3))(X)

    # CONV -> BN -> RELU Block applied to X
    X = Conv2D(64, (7, 7), strides = (1, 1))(X)
    X = BatchNormalization(axis = 3)(X)
    X = Activation('relu')(X)
    
    
    
    
    
    
    
    X = ZeroPadding2D((3, 3))(X)

    # CONV -> BN -> RELU Block applied to X
    X = Conv2D(64, (7, 7), strides = (1, 1))(X)
    X = BatchNormalization(axis = 3)(X)
    X = Activation('relu')(X)

    # MAXPOOL
    X = MaxPooling2D((3, 3))(X)
    
    
    
    

    
    X = ZeroPadding2D((3, 3))(X)

    # CONV -> BN -> RELU Block applied to X
    X = Conv2D(64, (7, 7), strides = (1, 1))(X)
    X = BatchNormalization(axis = 3)(X)
    X = Activation('relu')(X)

    # MAXPOOL
    X = MaxPooling2D((4, 4))(X)
    
    
    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED
    X = Flatten()(X)
    X = Dense(4, activation='linear', name='fc')(X)

    
    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.
    model = Model(inputs = X_input, outputs = X, name='HappyModel')

    ### END CODE HERE ###
    
    return model

from keras.optimizers import Adam
happyModel = HappyModel((128, 128, 1))
happyModel.compile(Adam(lr=0.032), loss='mean_squared_error' , metrics = ['accuracy'])
happyModel.summary()

file=pd.read_csv("/content/gdrive/My Drive/training.csv")
file=np.array(file)
X_train,X_test,Y_train,Y_test=train_test_split(file[:,0],file[:,1:5])
print(X_train.shape)
print(X_test.shape)

batch_s = 32
happyModel.fit_generator(
    generate_data(file[:,0],file[:,1:5], batch_s),
    samples_per_epoch=14000/batch_s , nb_epoch = 30)

preds = happyModel.evaluate_generator(generate_data(X_test , Y_test, batch_s), len(X_test)/batch_s)
#preds = happyModel.evaluate(x = X_test, y = Y_test)
### END CODE HERE ###
print()
print ("Loss = " + str(preds[0]))
print ("Test Accuracy = " + str(preds[1]))

# serialize model to JSON
model_json = happyModel.to_json()
with open("/content/gdrive/My Drive/model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
happyModel.save_weights("/content/gdrive/My Drive/model.h5")
print("Saved model to disk")

# load json and create model
json_file = open('/content/gdrive/My Drive/model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
# load weights into new model
loaded_model.load_weights("/content/gdrive/My Drive/model.h5")
print("Loaded model from disk")
 
# evaluate loaded model on test data
loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
score = loaded_model.evaluate(X, Y, verbose=0)
print("%s: %.2f%%" % (loaded_model.metrics_names[1], score[1]*100))

file_t = pd.read_csv("/content/gdrive/My Drive/test.csv")
file_t = np.array(file_t)
# happyModel.save_weights('/content/gdrive/My Drive/happymodel.h5')
print(file_t.shape)

predict = happyModel.predict_generator(generate_data_t(file_t[:, 0], 1),len(file_t)*1.0)
print(predict)

pre = predict*np.array([480, 640, 480, 640])

pre.shape

for i in range(0 , 12815):
  for j in range(0 , 4):
    if(pre[i, j] <= 0):
      pre[i, j] = 0

ans = np.concatenate((file_t[:,0].reshape(12815,1),pre),axis=1)

submission = pd.DataFrame(columns=['image_name', 'x1','x2','y1','y2'])
submission['image_name'] = pd.Series(ans[:,0].astype('str'))
submission['x1'] = pd.Series(pre[:,0].astype('int'))
submission['x2'] = pd.Series(pre[:,1].astype('int'))
submission['y1'] = pd.Series(pre[:,2].astype('int'))
submission['y2'] = pd.Series(pre[:,3].astype('int'))
submission.to_csv('/content/gdrive/My Drive/submission.csv', index=False)

happyModel.summary()

plot_model(happyModel, to_file='/content/gdrive/My Drive/HappyModel.png')
SVG(model_to_dot(happyModel).create(prog='dot', format='svg'))

happyModel.save('/content/gdrive/My Drive/happymodel.h5')

from keras.models import load_model
newmodel = load_model('/content/gdrive/My Drive/happymodel.h5')
newmodel.summary()

newmodel.get_weights()

newmodel.optimizer

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import matplotlib.patches as patches 
# %matplotlib inline

traindf = pd.read_csv('/content/gdrive/My Drive/submission.csv', sep=',')
traindf.head(3)

trainimage = traindf.loc[:,'image_name'][:500]
train10=list(trainimage[:498])
print(train10[:3])

import glob
def load_image_and_box(dir_name = '/content/gdrive/My Drive/testingimages'):
        
    training = {}
    
    for name in glob.glob(dir_name + '/*.png'):
        
        if name[39:] in train10:
          
            training[name[39:]] = plt.imread(name)

    return training
    
    

training_imgs = load_image_and_box()

training_imgs['1474723840903DSC08089.png']

def visualize(imgs,rect, format=None):
    fig=plt.figure(figsize=(30,10*(len(imgs)/3)))
    for i, img in enumerate(imgs):
        if img.shape[0] == 3:
            img = img.transpose(1,2,0)
        plt_idx = i+1
        ax=fig.add_subplot(len(imgs)/3, 3, plt_idx)
        rec=patches.Rectangle((rect[i][0][0],rect[i][0][2]),rect[i][0][1]-rect[i][0][0],rect[i][0][3]-rect[i][0][2],linewidth=1,edgecolor='r',facecolor='None')
        ax.add_patch(rec)
        plt.imshow(img, cmap=format)
    plt.show()


data = [] 
rect = []
for key,value in training_imgs.items():
    data.append(value)
    rect.append(np.array(traindf[traindf['image_name'] == key].loc[:,['x1','x2','y1','y2']].values))
visualize(data[:99],rect[:99])

"""%%HTML
<iframe width='100%' height='350' src='https://google.com'></iframe>
"""

import webbrowser

# generate an URL
url = 'https://' + 'www.google.com'
webbrowser.open(url)

